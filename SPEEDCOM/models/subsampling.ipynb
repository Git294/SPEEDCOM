{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yang\\.conda\\envs\\test_speedcom\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataUtils import DataUtils as utils\n",
    "from model_utils import ModelUtils as mutls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, Conv1D, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   #      Name      name_smiles  Wavelength  Epsilon  Quantum Yield\n",
      "0  1   Benzene      C1=CC=CC=C1      254.75      210          0.053\n",
      "1  2   Toluene     CC1=CC=CC=C1      261.75     2860          0.170\n",
      "2  3  o-Xylene    CC1=CC=CC=C1C      263.00      254          0.170\n",
      "3  4  m-Xylene  CC1=CC(=CC=C1)C      265.00      284          0.130\n",
      "4  5  p-Xylene  CC1=CC=C(C=C1)C      275.00      770          0.220\n"
     ]
    }
   ],
   "source": [
    "dset = utils.readData('temp_cleaned_data.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = utils.get_xy(dset, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([91., 57., 18., 25., 21., 23., 18.,  7.,  3.,  4.]),\n",
       " array([203. , 264.5, 326. , 387.5, 449. , 510.5, 572. , 633.5, 695. ,\n",
       "        756.5, 818. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADelJREFUeJzt3W2spHV5x/Hvr6yo4MPycDQra3ogIbamaYVsKEhjGrAPihFeYIIx7drQkPTRhya6tkmN76Axaps02o3UkMYqFGkhYGsJ4ov2xdpdQAFXyopbXEE4tqKtb5R69cX8V07Xs5zZPefsnLn4fpKTue///If7unZnfnuf/9wzpKqQJM2/n5p1AZKk9WGgS1ITBrokNWGgS1ITBrokNWGgS1ITBrokNWGgS1ITBrokNbHlRB7szDPPrMXFxRN5SEmae/v27ft2VS2sNu+EBvri4iJ79+49kYeUpLmX5D+mmeeSiyQ1YaBLUhMGuiQ1YaBLUhMGuiQ1YaBLUhMGuiQ1YaBLUhMGuiQ1cUI/KboWi7vumMlxD1572UyOK0nHyjN0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJqYK9CTvSvJgkgeSfCrJC5KcnWRPkoeT3Jjk5I0uVpJ0dKsGepKzgD8EdlTVzwEnAVcB1wEfrqpzge8AV29koZKkZzftkssW4IVJtgCnAI8DlwA3j/tvAK5Y//IkSdNaNdCr6pvAB4FHmQT5d4F9wFNV9fSYdgg4a6OKlCStbpoll9OAy4GzgVcApwJvWGFqHeXx1yTZm2Tv0tLSWmqVJD2LaZZcXg98vaqWquqHwC3Aa4GtYwkGYDvw2EoPrqrdVbWjqnYsLCysS9GSpJ80TaA/ClyY5JQkAS4FvgLcDVw55uwEbt2YEiVJ05hmDX0Pkzc/7wHuH4/ZDbwXeHeSA8AZwPUbWKckaRVbVp8CVfV+4P1HDD8CXLDuFUmSjoufFJWkJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJgx0SWrCQJekJqYK9CRbk9yc5KtJ9ie5KMnpSe5M8vC4PW2ji5UkHd20Z+h/DvxTVf0M8AvAfmAXcFdVnQvcNfYlSTOyaqAneQnwOuB6gKr6QVU9BVwO3DCm3QBcsVFFSpJWN80Z+jnAEvCJJPcm+XiSU4GXV9XjAOP2ZSs9OMk1SfYm2bu0tLRuhUuS/r9pAn0LcD7w0ao6D/g+x7C8UlW7q2pHVe1YWFg4zjIlSauZJtAPAYeqas/Yv5lJwD+RZBvAuH1yY0qUJE1j1UCvqm8B30jyqjF0KfAV4DZg5xjbCdy6IRVKkqayZcp5fwB8MsnJwCPAbzH5x+CmJFcDjwJv2ZgSJUnTmCrQq+o+YMcKd126vuVIko6XnxSVpCamXXJ5zlrcdcfMjn3w2stmdmxJ88czdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCamDvQkJyW5N8ntY//sJHuSPJzkxiQnb1yZkqTVHMsZ+juA/cv2rwM+XFXnAt8Brl7PwiRJx2aqQE+yHbgM+PjYD3AJcPOYcgNwxUYUKEmazrRn6B8B3gP8aOyfATxVVU+P/UPAWetcmyTpGKwa6EneBDxZVfuWD68wtY7y+GuS7E2yd2lp6TjLlCStZpoz9IuBNyc5CHyayVLLR4CtSbaMOduBx1Z6cFXtrqodVbVjYWFhHUqWJK1k1UCvqvdV1faqWgSuAj5fVW8D7gauHNN2ArduWJWSpFWt5Tr09wLvTnKAyZr69etTkiTpeGxZfcozquoLwBfG9iPABetfkiTpePhJUUlqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqYsusC9Dms7jrjpkd++C1l83s2NK88wxdkpow0CWpCQNdkppwDV3C9w3Ug2foktSEgS5JTRjoktTEqoGe5JVJ7k6yP8mDSd4xxk9PcmeSh8ftaRtfriTpaKZ5U/Rp4I+q6p4kLwb2JbkTeDtwV1Vdm2QXsAt478aVqueCWb45Kc27Vc/Qq+rxqrpnbP83sB84C7gcuGFMuwG4YqOKlCSt7pjW0JMsAucBe4CXV9XjMAl94GVHecw1SfYm2bu0tLS2aiVJRzV1oCd5EfAZ4J1V9b1pH1dVu6tqR1XtWFhYOJ4aJUlTmCrQkzyPSZh/sqpuGcNPJNk27t8GPLkxJUqSpjHNVS4Brgf2V9WHlt11G7BzbO8Ebl3/8iRJ05rmKpeLgd8A7k9y3xj7Y+Ba4KYkVwOPAm/ZmBIlSdNYNdCr6l+AHOXuS9e3HC3nJXzPDbP6e/Y7ZPrxk6KS1ISBLklNGOiS1ISBLklNGOiS1ISBLklNGOiS1ISBLklNGOiS1ISBLklNGOiS1ISBLklNGOiS1ISBLklNGOiS1ISBLklNGOiS1ISBLklNGOiS1ISBLklNGOiS1ISBLklNGOiS1ISBLklNbJl1AZJmY3HXHTM79sFrL5vZsTvzDF2SmjDQJakJA12SmjDQJakJA12SmjDQJakJL1uU9Jwxq0s1T9Rlmp6hS1ITnqFLOuFm+aGmzjxDl6QmDHRJamJNgZ7k15M8lORAkl3rVZQk6dgdd6AnOQn4S+ANwKuBtyZ59XoVJkk6Nms5Q78AOFBVj1TVD4BPA5evT1mSpGO1lkA/C/jGsv1DY0ySNANruWwxK4zVT0xKrgGuGbv/k+Sh4zjWmcC3j+Nxm0mHHqBHHx16gB59PCd6yHVrPsZPTzNpLYF+CHjlsv3twGNHTqqq3cDuNRyHJHurasda/huz1qEH6NFHhx6gRx/2sL7WsuTyb8C5Sc5OcjJwFXDb+pQlSTpWx32GXlVPJ/l94HPAScBfV9WD61aZJOmYrOmj/1X1WeCz61TLs1nTks0m0aEH6NFHhx6gRx/2sI5S9RPvY0qS5pAf/ZekJmYe6ElemeTuJPuTPJjkHWP89CR3Jnl43J42xpPkL8bXDXw5yfmz7QCSvCDJF5N8afTwgTF+dpI9o4cbx5vHJHn+2D8w7l+cZf1HSnJSknuT3D7256qPJAeT3J/kviR7x9jcPJ8OS7I1yc1JvjpeHxfNUx9JXjX+Dg7/fC/JO+eph8OSvGu8th9I8qnxmt98r4uqmukPsA04f2y/GPh3Jl8l8GfArjG+C7hubL8R+Ecm18FfCOzZBD0EeNHYfh6wZ9R2E3DVGP8Y8Dtj+3eBj43tq4AbZ93DEf28G/hb4PaxP1d9AAeBM48Ym5vn07KabwB+e2yfDGydxz5GfScB32JyPfVc9cDkA5NfB1449m8C3r4ZXxcz/8Na4Q/vVuBXgIeAbWNsG/DQ2P4r4K3L5v943mb4AU4B7gF+kcmHDbaM8YuAz43tzwEXje0tY15mXfuoZztwF3AJcPt4cc1VH0cJ9Ll6PgEvGSGSI8bnqo9l9fwq8K/z2APPfCr+9PE8vx34tc34upj5ksty41eT85ic4b68qh4HGLcvG9M25VcOjGWK+4AngTuBrwFPVdXTY8ryOn/cw7j/u8AZJ7bio/oI8B7gR2P/DOavjwL+Ocm+TD6pDHP2fALOAZaAT4zlr48nOZX56+Owq4BPje256qGqvgl8EHgUeJzJ83wfm/B1sWkCPcmLgM8A76yq7z3b1BXGZn6pTlX9b1W9hskZ7gXAz640bdxuyh6SvAl4sqr2LR9eYeqm7gO4uKrOZ/JNoL+X5HXPMnez9rAFOB/4aFWdB3yfyfLE0WzWPhhry28G/m61qSuMzbyHscZ/OXA28ArgVCbPrSPN/HWxKQI9yfOYhPknq+qWMfxEkm3j/m1Mznxhyq8cmJWqegr4ApM1wK1JDl/rv7zOH/cw7n8p8F8nttIVXQy8OclBJt+eeQmTM/a56qOqHhu3TwJ/z+Qf2Hl7Ph0CDlXVnrF/M5OAn7c+YBJ+91TVE2N/3np4PfD1qlqqqh8CtwCvZRO+LmYe6EkCXA/sr6oPLbvrNmDn2N7JZG398PhvjnfELwS+e/jXt1lJspBk69h+IZMnwH7gbuDKMe3IHg73diXw+RoLbrNUVe+rqu1VtcjkV+TPV9XbmKM+kpya5MWHt5ms3T7AHD2fAKrqW8A3krxqDF0KfIU562N4K88st8D89fAocGGSU0ZeHf672Hyvi03whsMvMfl15MvAfePnjUzWnO4CHh63p4/5YfI/1vgacD+wYxP08PPAvaOHB4A/HePnAF8EDjD5dfP5Y/wFY//AuP+cWfewQk+/zDNXucxNH6PWL42fB4E/GeNz83xa1strgL3jefUPwGnz1geTiwT+E3jpsrG56mHU9gHgq+P1/TfA8zfj68JPikpSEzNfcpEkrQ8DXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKa+D/dgX8VxH0i+AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(list(dset[:,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_category = (Y - 200) // 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([81., 65., 18., 25., 18., 25., 18.,  9.,  4.,  4.]),\n",
       " array([ 0.,  3.,  6.,  9., 12., 15., 18., 21., 24., 27., 30.]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(list(Y_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cat=mutls.combine_columns((X,Y_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 18, 1.0: 23, 2.0: 40, 3.0: 44, 4.0: 14, 5.0: 7, 6.0: 8, 7.0: 4, 8.0: 6, 9.0: 3, 10.0: 12, 11.0: 10, 12.0: 7, 13.0: 4, 14.0: 7, 15.0: 10, 16.0: 5, 17.0: 10, 18.0: 6, 19.0: 6, 20.0: 6, 21.0: 3, 22.0: 4, 23.0: 2, 24.0: 2, 25.0: 2, 28.0: 1, 29.0: 2, 30.0: 1}\n"
     ]
    }
   ],
   "source": [
    "cls, counts = np.unique(Y_category, return_counts=True)\n",
    "cls_counts = dict(zip(cls, counts))\n",
    "print(cls_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subsample the data in each class to undersample some class with big counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled=mutls.subsampling(d_cat,1,10,cls_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1=CC=CC=C1\n",
      "2.0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for row in d_cat:\n",
    "    print(row[0])\n",
    "    print(row[1])\n",
    "    cls = row[1]\n",
    "    print(np.random.randint(0, cls_counts[cls]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 8., 12., 15., 11., 12.,  7.,  8.,  4.,  6.,  3., 11., 10.,  7.,\n",
       "         4.,  7., 10.,  5., 10.,  6.,  6.,  6.,  3.,  4.,  2.,  2.,  2.,\n",
       "         0.,  0.,  1.,  2.,  1.]),\n",
       " array([ 0.        ,  0.96774194,  1.93548387,  2.90322581,  3.87096774,\n",
       "         4.83870968,  5.80645161,  6.77419355,  7.74193548,  8.70967742,\n",
       "         9.67741935, 10.64516129, 11.61290323, 12.58064516, 13.5483871 ,\n",
       "        14.51612903, 15.48387097, 16.4516129 , 17.41935484, 18.38709677,\n",
       "        19.35483871, 20.32258065, 21.29032258, 22.25806452, 23.22580645,\n",
       "        24.19354839, 25.16129032, 26.12903226, 27.09677419, 28.06451613,\n",
       "        29.03225806, 30.        ]),\n",
       " <a list of 31 Patch objects>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(list(subsampled[:, 1]), bins=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166, 276)\n",
      "(19, 276)\n",
      "(166, 31)\n",
      "(19, 31)\n"
     ]
    }
   ],
   "source": [
    "X, y = subsampled[:,0], subsampled[:,1]\n",
    "word_map = utils.get_wordmap(X)\n",
    "embed_length = utils.get_max_len(X) + 2\n",
    "X_numeric = utils.numeric_encoding(x_list=X, uniform_length=embed_length, word_map=word_map)\n",
    "y_onehot = mutls.onehot_encode_y(y=y, num_class=31)\n",
    "X_train, X_test, y_train, y_test = utils.splitData(X_numeric, y_onehot, ratio=0.10)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model1: embedding layer with category data and rmsprop optimizer (slow and bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Yang\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Yang\\.conda\\envs\\test_speedcom\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "num_filter = 192\n",
    "kernel_size = 10\n",
    "rnn1_size = 224\n",
    "rnn2_size = 384\n",
    "dropout_rate = 0.4\n",
    "\n",
    "\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "num_classes = 31\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_map), embedding_dim, input_length=embed_length))\n",
    "model.add(Conv1D(num_filter, kernel_size, activation='relu'))\n",
    "model.add(LSTM(rnn1_size, return_sequences=True, dropout=dropout_rate))\n",
    "model.add(LSTM(rnn2_size))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Yang\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 166 samples, validate on 19 samples\n",
      "Epoch 1/50\n",
      "166/166 [==============================] - 36s 214ms/step - loss: 3.4277 - acc: 0.0361 - val_loss: 3.4629 - val_acc: 0.2105\n",
      "Epoch 2/50\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 3.4843 - acc: 0.0361 - val_loss: 3.2147 - val_acc: 0.0526\n",
      "Epoch 3/50\n",
      "166/166 [==============================] - 30s 180ms/step - loss: 3.3111 - acc: 0.0542 - val_loss: 3.1872 - val_acc: 0.2105\n",
      "Epoch 4/50\n",
      "166/166 [==============================] - 29s 178ms/step - loss: 3.2832 - acc: 0.0542 - val_loss: 3.1552 - val_acc: 0.2105\n",
      "Epoch 5/50\n",
      "166/166 [==============================] - 29s 177ms/step - loss: 3.2932 - acc: 0.0301 - val_loss: 3.2062 - val_acc: 0.0526\n",
      "Epoch 6/50\n",
      "166/166 [==============================] - 30s 178ms/step - loss: 3.2602 - acc: 0.0482 - val_loss: 3.2369 - val_acc: 0.0526\n",
      "Epoch 7/50\n",
      "166/166 [==============================] - 30s 179ms/step - loss: 3.2523 - acc: 0.0482 - val_loss: 3.2860 - val_acc: 0.0526\n",
      "Epoch 8/50\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 3.2607 - acc: 0.0422 - val_loss: 3.2217 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "166/166 [==============================] - 30s 181ms/step - loss: 3.2576 - acc: 0.0542 - val_loss: 3.2437 - val_acc: 0.0526\n",
      "Epoch 10/50\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 3.2382 - acc: 0.0361 - val_loss: 3.2460 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "166/166 [==============================] - 30s 180ms/step - loss: 3.2167 - acc: 0.0723 - val_loss: 3.2834 - val_acc: 0.0526\n",
      "Epoch 12/50\n",
      "166/166 [==============================] - 30s 181ms/step - loss: 3.2297 - acc: 0.0723 - val_loss: 3.2509 - val_acc: 0.0526\n",
      "Epoch 13/50\n",
      "166/166 [==============================] - 30s 182ms/step - loss: 3.2244 - acc: 0.0542 - val_loss: 3.1965 - val_acc: 0.2105\n",
      "Epoch 14/50\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 3.2211 - acc: 0.0783 - val_loss: 3.2159 - val_acc: 0.0526\n",
      "Epoch 15/50\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 3.2122 - acc: 0.0542 - val_loss: 3.1921 - val_acc: 0.0526\n",
      "Epoch 16/50\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 3.2144 - acc: 0.0542 - val_loss: 3.1475 - val_acc: 0.2105\n",
      "Epoch 17/50\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 3.2083 - acc: 0.0723 - val_loss: 3.1255 - val_acc: 0.2105\n",
      "Epoch 18/50\n",
      "166/166 [==============================] - 19s 116ms/step - loss: 3.2213 - acc: 0.0723 - val_loss: 3.1417 - val_acc: 0.2105\n",
      "Epoch 19/50\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 3.2155 - acc: 0.0602 - val_loss: 3.2238 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "166/166 [==============================] - 19s 115ms/step - loss: 3.2099 - acc: 0.0422 - val_loss: 3.1450 - val_acc: 0.2105\n",
      "Epoch 21/50\n",
      "166/166 [==============================] - 22s 131ms/step - loss: 3.2106 - acc: 0.0602 - val_loss: 3.2018 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 3.2031 - acc: 0.0542 - val_loss: 3.2408 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "166/166 [==============================] - 30s 178ms/step - loss: 3.2094 - acc: 0.0422 - val_loss: 3.2024 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 3.2024 - acc: 0.0723 - val_loss: 3.1776 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "166/166 [==============================] - 30s 181ms/step - loss: 3.2073 - acc: 0.0723 - val_loss: 3.1418 - val_acc: 0.2105\n",
      "Epoch 26/50\n",
      "166/166 [==============================] - 32s 191ms/step - loss: 7.4001 - acc: 0.0301 - val_loss: 3.1723 - val_acc: 0.2105\n",
      "Epoch 27/50\n",
      "166/166 [==============================] - 32s 192ms/step - loss: 3.1966 - acc: 0.0723 - val_loss: 3.1757 - val_acc: 0.0526\n",
      "Epoch 28/50\n",
      "166/166 [==============================] - 31s 188ms/step - loss: 3.1965 - acc: 0.0542 - val_loss: 3.1799 - val_acc: 0.0526\n",
      "Epoch 29/50\n",
      "166/166 [==============================] - 32s 190ms/step - loss: 3.1920 - acc: 0.0542 - val_loss: 3.1980 - val_acc: 0.0526\n",
      "Epoch 30/50\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 3.1944 - acc: 0.0482 - val_loss: 3.2070 - val_acc: 0.0526\n",
      "Epoch 31/50\n",
      "166/166 [==============================] - 33s 197ms/step - loss: 3.1894 - acc: 0.0723 - val_loss: 3.2326 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "166/166 [==============================] - 30s 180ms/step - loss: 3.1895 - acc: 0.0723 - val_loss: 3.2561 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "166/166 [==============================] - 19s 115ms/step - loss: 3.1927 - acc: 0.0723 - val_loss: 3.2605 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "166/166 [==============================] - 18s 105ms/step - loss: 3.1947 - acc: 0.0723 - val_loss: 3.2643 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "166/166 [==============================] - 27s 164ms/step - loss: 3.1942 - acc: 0.0783 - val_loss: 3.2189 - val_acc: 0.0526\n",
      "Epoch 36/50\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 3.2012 - acc: 0.0602 - val_loss: 3.2084 - val_acc: 0.1579\n",
      "Epoch 37/50\n",
      "166/166 [==============================] - 30s 180ms/step - loss: 3.1934 - acc: 0.0361 - val_loss: 3.2244 - val_acc: 0.0526\n",
      "Epoch 38/50\n",
      "166/166 [==============================] - 31s 187ms/step - loss: 3.1977 - acc: 0.0663 - val_loss: 3.2192 - val_acc: 0.0526\n",
      "Epoch 39/50\n",
      "166/166 [==============================] - 30s 180ms/step - loss: 3.1937 - acc: 0.0602 - val_loss: 3.2353 - val_acc: 0.0526\n",
      "Epoch 40/50\n",
      "166/166 [==============================] - 30s 182ms/step - loss: 3.1998 - acc: 0.0602 - val_loss: 3.2260 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "166/166 [==============================] - 30s 180ms/step - loss: 3.1964 - acc: 0.0723 - val_loss: 3.2334 - val_acc: 0.0526\n",
      "Epoch 42/50\n",
      "166/166 [==============================] - 31s 187ms/step - loss: 3.1968 - acc: 0.0723 - val_loss: 3.2491 - val_acc: 0.0526\n",
      "Epoch 43/50\n",
      "166/166 [==============================] - 30s 183ms/step - loss: 3.1975 - acc: 0.0723 - val_loss: 3.1872 - val_acc: 0.2105\n",
      "Epoch 44/50\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 3.1945 - acc: 0.0723 - val_loss: 3.2169 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 3.1940 - acc: 0.0723 - val_loss: 3.2203 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "166/166 [==============================] - 20s 121ms/step - loss: 3.1932 - acc: 0.0602 - val_loss: 3.2177 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "166/166 [==============================] - 19s 117ms/step - loss: 3.1976 - acc: 0.0602 - val_loss: 3.2306 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "166/166 [==============================] - 19s 116ms/step - loss: 3.1953 - acc: 0.0361 - val_loss: 3.2104 - val_acc: 0.2105\n",
      "Epoch 49/50\n",
      "166/166 [==============================] - 19s 113ms/step - loss: 3.1944 - acc: 0.0422 - val_loss: 3.2076 - val_acc: 0.2105\n",
      "Epoch 50/50\n",
      "166/166 [==============================] - 18s 111ms/step - loss: 3.1963 - acc: 0.0301 - val_loss: 3.2120 - val_acc: 0.2105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x179c8778eb8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model2 no imbedding, adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0],X_train.shape[1],1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "rnn1_size = 224\n",
    "rnn2_size = 384\n",
    "dropout_rate = 0.4\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "num_classes = 31\n",
    "optimizer = 'adam' #use adam optimizer\n",
    "\n",
    "model_1 = Sequential()\n",
    "model_1.add(LSTM(rnn1_size, return_sequences=True, input_shape=(276,1))) #276\n",
    "model_1.add(LSTM(rnn2_size))\n",
    "model_1.add(Dense(num_classes, activation='softmax'))\n",
    "model_1.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 166 samples, validate on 19 samples\n",
      "Epoch 1/50\n",
      "166/166 [==============================] - 18s 111ms/step - loss: 3.4318 - acc: 0.0301 - val_loss: 3.2489 - val_acc: 0.0526\n",
      "Epoch 2/50\n",
      "166/166 [==============================] - 16s 96ms/step - loss: 3.2947 - acc: 0.0482 - val_loss: 3.2740 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "166/166 [==============================] - 16s 99ms/step - loss: 3.2554 - acc: 0.0663 - val_loss: 3.2637 - val_acc: 0.0526\n",
      "Epoch 4/50\n",
      "166/166 [==============================] - 17s 101ms/step - loss: 3.2651 - acc: 0.0663 - val_loss: 3.2877 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "166/166 [==============================] - 17s 104ms/step - loss: 3.2373 - acc: 0.0663 - val_loss: 3.3340 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "166/166 [==============================] - 17s 104ms/step - loss: 3.2375 - acc: 0.0482 - val_loss: 3.3037 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "166/166 [==============================] - 17s 100ms/step - loss: 3.2291 - acc: 0.0663 - val_loss: 3.2345 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "166/166 [==============================] - 17s 100ms/step - loss: 3.2217 - acc: 0.0663 - val_loss: 3.2177 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "166/166 [==============================] - 17s 105ms/step - loss: 3.2168 - acc: 0.0783 - val_loss: 3.2484 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "166/166 [==============================] - 17s 105ms/step - loss: 3.2167 - acc: 0.0482 - val_loss: 3.2876 - val_acc: 0.0526\n",
      "Epoch 11/50\n",
      "166/166 [==============================] - 17s 104ms/step - loss: 3.2217 - acc: 0.0663 - val_loss: 3.2550 - val_acc: 0.0526\n",
      "Epoch 12/50\n",
      "166/166 [==============================] - 18s 108ms/step - loss: 3.2252 - acc: 0.0482 - val_loss: 3.2296 - val_acc: 0.2105\n",
      "Epoch 13/50\n",
      "166/166 [==============================] - 17s 104ms/step - loss: 3.2191 - acc: 0.0663 - val_loss: 3.2165 - val_acc: 0.2105\n",
      "Epoch 14/50\n",
      "166/166 [==============================] - 18s 106ms/step - loss: 3.2149 - acc: 0.0663 - val_loss: 3.2059 - val_acc: 0.2105\n",
      "Epoch 15/50\n",
      "166/166 [==============================] - 17s 104ms/step - loss: 3.2110 - acc: 0.0663 - val_loss: 3.2134 - val_acc: 0.2105\n",
      "Epoch 16/50\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 3.2157 - acc: 0.0663 - val_loss: 3.2077 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 3.2156 - acc: 0.0602 - val_loss: 3.2297 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "166/166 [==============================] - 18s 109ms/step - loss: 3.2066 - acc: 0.0723 - val_loss: 3.2007 - val_acc: 0.2105\n",
      "Epoch 19/50\n",
      "166/166 [==============================] - 18s 108ms/step - loss: 3.2086 - acc: 0.0663 - val_loss: 3.1519 - val_acc: 0.2105\n",
      "Epoch 20/50\n",
      "166/166 [==============================] - 19s 113ms/step - loss: 3.2151 - acc: 0.0663 - val_loss: 3.1504 - val_acc: 0.2105\n",
      "Epoch 21/50\n",
      "166/166 [==============================] - 16s 94ms/step - loss: 3.2146 - acc: 0.0663 - val_loss: 3.1721 - val_acc: 0.2105\n",
      "Epoch 22/50\n",
      "166/166 [==============================] - 16s 98ms/step - loss: 3.2061 - acc: 0.0663 - val_loss: 3.1830 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "166/166 [==============================] - 16s 95ms/step - loss: 3.2125 - acc: 0.0663 - val_loss: 3.2088 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "166/166 [==============================] - 16s 99ms/step - loss: 3.2104 - acc: 0.0723 - val_loss: 3.1723 - val_acc: 0.2105\n",
      "Epoch 25/50\n",
      "166/166 [==============================] - 19s 116ms/step - loss: 3.2136 - acc: 0.0663 - val_loss: 3.1497 - val_acc: 0.2105\n",
      "Epoch 26/50\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 3.2054 - acc: 0.0663 - val_loss: 3.1878 - val_acc: 0.2105\n",
      "Epoch 27/50\n",
      "166/166 [==============================] - 15s 92ms/step - loss: 3.2101 - acc: 0.0602 - val_loss: 3.2318 - val_acc: 0.0526\n",
      "Epoch 28/50\n",
      "166/166 [==============================] - 15s 93ms/step - loss: 3.2063 - acc: 0.0663 - val_loss: 3.2386 - val_acc: 0.0526\n",
      "Epoch 29/50\n",
      "166/166 [==============================] - 16s 99ms/step - loss: 3.2088 - acc: 0.0663 - val_loss: 3.2414 - val_acc: 0.0526\n",
      "Epoch 30/50\n",
      "166/166 [==============================] - 16s 98ms/step - loss: 3.2066 - acc: 0.0663 - val_loss: 3.2131 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "166/166 [==============================] - 17s 104ms/step - loss: 3.2013 - acc: 0.0663 - val_loss: 3.2064 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "166/166 [==============================] - 16s 95ms/step - loss: 3.2073 - acc: 0.0663 - val_loss: 3.2154 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "166/166 [==============================] - 16s 94ms/step - loss: 3.2063 - acc: 0.0663 - val_loss: 3.1901 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "166/166 [==============================] - 16s 96ms/step - loss: 3.2068 - acc: 0.0783 - val_loss: 3.1601 - val_acc: 0.2105\n",
      "Epoch 35/50\n",
      "166/166 [==============================] - 16s 94ms/step - loss: 3.2072 - acc: 0.0663 - val_loss: 3.1308 - val_acc: 0.2105\n",
      "Epoch 36/50\n",
      "166/166 [==============================] - 16s 94ms/step - loss: 3.2100 - acc: 0.0663 - val_loss: 3.1126 - val_acc: 0.2105\n",
      "Epoch 37/50\n",
      "166/166 [==============================] - 16s 98ms/step - loss: 3.2064 - acc: 0.0663 - val_loss: 3.1409 - val_acc: 0.2105\n",
      "Epoch 38/50\n",
      "166/166 [==============================] - 16s 97ms/step - loss: 3.2030 - acc: 0.0783 - val_loss: 3.1695 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "166/166 [==============================] - 16s 95ms/step - loss: 3.2068 - acc: 0.0663 - val_loss: 3.2004 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "166/166 [==============================] - 16s 96ms/step - loss: 3.2125 - acc: 0.0663 - val_loss: 3.2281 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "166/166 [==============================] - 16s 94ms/step - loss: 3.2095 - acc: 0.0663 - val_loss: 3.2112 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 3.2057 - acc: 0.0663 - val_loss: 3.2175 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "166/166 [==============================] - 16s 95ms/step - loss: 3.2039 - acc: 0.0602 - val_loss: 3.2229 - val_acc: 0.0526\n",
      "Epoch 44/50\n",
      "166/166 [==============================] - 16s 95ms/step - loss: 3.2039 - acc: 0.0542 - val_loss: 3.2193 - val_acc: 0.0526\n",
      "Epoch 45/50\n",
      "166/166 [==============================] - 16s 97ms/step - loss: 3.2048 - acc: 0.0663 - val_loss: 3.2365 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "166/166 [==============================] - 16s 95ms/step - loss: 3.2092 - acc: 0.0663 - val_loss: 3.2553 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "166/166 [==============================] - 16s 99ms/step - loss: 3.2083 - acc: 0.0482 - val_loss: 3.2260 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "166/166 [==============================] - 16s 98ms/step - loss: 3.2090 - acc: 0.0663 - val_loss: 3.1941 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "166/166 [==============================] - 16s 96ms/step - loss: 3.2014 - acc: 0.0663 - val_loss: 3.1979 - val_acc: 0.0526\n",
      "Epoch 50/50\n",
      "166/166 [==============================] - 16s 97ms/step - loss: 3.2010 - acc: 0.0663 - val_loss: 3.2023 - val_acc: 0.0526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x179c87782b0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "model_1.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 276, 224)          202496    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 384)               935424    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 31)                11935     \n",
      "=================================================================\n",
      "Total params: 1,149,855\n",
      "Trainable params: 1,149,855\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 166 samples, validate on 19 samples\n",
      "Epoch 1/100\n",
      "166/166 [==============================] - 15s 92ms/step - loss: 3.2025 - acc: 0.0663 - val_loss: 3.2026 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "166/166 [==============================] - 16s 96ms/step - loss: 3.2051 - acc: 0.0602 - val_loss: 3.2025 - val_acc: 0.2105\n",
      "Epoch 3/100\n",
      "166/166 [==============================] - 15s 93ms/step - loss: 3.2032 - acc: 0.0663 - val_loss: 3.2192 - val_acc: 0.2105\n",
      "Epoch 4/100\n",
      "166/166 [==============================] - 16s 93ms/step - loss: 3.2092 - acc: 0.0783 - val_loss: 3.2435 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "166/166 [==============================] - 15s 93ms/step - loss: 3.2092 - acc: 0.0663 - val_loss: 3.2617 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "166/166 [==============================] - 15s 92ms/step - loss: 3.2055 - acc: 0.0783 - val_loss: 3.2557 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "166/166 [==============================] - 15s 93ms/step - loss: 3.2037 - acc: 0.0663 - val_loss: 3.2394 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "166/166 [==============================] - 17s 101ms/step - loss: 3.2031 - acc: 0.0663 - val_loss: 3.2249 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "128/166 [======================>.......] - ETA: 3s - loss: 3.1783 - acc: 0.0547"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b47e137dc673>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\test_speedcom\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\.conda\\envs\\test_speedcom\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\test_speedcom\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\test_speedcom\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_1.fit(X_train, y_train, batch_size=batch_size, epochs=100, validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_speedcom",
   "language": "python",
   "name": "test_speedcom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
